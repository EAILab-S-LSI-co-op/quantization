{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:375: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.vgg import *\n",
    "from src.util import *\n",
    "\n",
    "def quantize_model(model):\n",
    "    quantize_model = copy.deepcopy(model)\n",
    "    quantize_model.qconfig = torch.quantization.get_default_qconfig()\n",
    "    quantize_model = torch.quantization.QuantWrapper(quantize_model)\n",
    "    torch.quantization.prepare(quantize_model, inplace=True)\n",
    "    torch.quantization.convert(quantize_model, inplace=True)\n",
    "    return quantize_model\n",
    "\n",
    "model_path = \"./models\"\n",
    "vgg11 = VGG11(); vgg11.load_state_dict(torch.load(f'{model_path}/VGG11.pth')); vgg11.eval()\n",
    "vgg13 = VGG13(); vgg13.load_state_dict(torch.load(f'{model_path}/VGG13.pth')); vgg13.eval()\n",
    "vgg16 = VGG16(); vgg16.load_state_dict(torch.load(f'{model_path}/VGG16.pth')); vgg16.eval()\n",
    "vgg19 = VGG19(); vgg19.load_state_dict(torch.load(f'{model_path}/VGG19.pth')); vgg19.eval()\n",
    "\n",
    "q_vgg11 = quantize_model(vgg11); q_vgg11.load_state_dict(torch.load(f'{model_path}/quantized_VGG11.pth'))\n",
    "q_vgg13 = quantize_model(vgg13); q_vgg13.load_state_dict(torch.load(f'{model_path}/quantized_VGG13.pth'))\n",
    "q_vgg16 = quantize_model(vgg16); q_vgg16.load_state_dict(torch.load(f'{model_path}/quantized_VGG16.pth'))\n",
    "q_vgg19 = quantize_model(vgg19); q_vgg19.load_state_dict(torch.load(f'{model_path}/quantized_VGG19.pth'))\n",
    "\n",
    "models = {\n",
    "    'vgg11': vgg11,\n",
    "    'vgg13': vgg13,\n",
    "    'vgg16': vgg16,\n",
    "    'vgg19': vgg19,\n",
    "    'q_vgg11': q_vgg11,\n",
    "    'q_vgg13': q_vgg13,\n",
    "    'q_vgg16': q_vgg16,\n",
    "    'q_vgg19': q_vgg19\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg11 elapsed time:\t 0.001895\n",
      "vgg13 elapsed time:\t 0.002358\n",
      "vgg16 elapsed time:\t 0.003296\n",
      "vgg19 elapsed time:\t 0.004157\n",
      "q_vgg11 elapsed time:\t 0.000844\n",
      "q_vgg13 elapsed time:\t 0.001038\n",
      "q_vgg16 elapsed time:\t 0.001449\n",
      "q_vgg19 elapsed time:\t 0.002144\n"
     ]
    }
   ],
   "source": [
    "# measure inference latency\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    elapsed_time = measure_inference_latency(model, 'cpu')\n",
    "    print(f'{model_name} elapsed time:\\t {elapsed_time:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg11 model size:\t 35.2350 MB\n",
      "vgg13 model size:\t 35.9418 MB\n",
      "vgg16 model size:\t 56.2162 MB\n",
      "vgg19 model size:\t 76.4906 MB\n",
      "q_vgg11 model size:\t 0.0422 MB\n",
      "q_vgg13 model size:\t 0.0451 MB\n",
      "q_vgg16 model size:\t 0.0647 MB\n",
      "q_vgg19 model size:\t 0.0843 MB\n"
     ]
    }
   ],
   "source": [
    "# measure model size\n",
    "scale = 1024**2\n",
    "for model_name, model in models.items():\n",
    "    model_size = measure_model_size(model)\n",
    "    print(f'{model_name} model size:\\t {model_size/scale:.4f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "vgg11 accuracy:\t 0.8568\n",
      "vgg13 accuracy:\t 0.8719\n",
      "vgg16 accuracy:\t 0.8728\n",
      "vgg19 accuracy:\t 0.8762\n",
      "q_vgg11 accuracy:\t 0.8566\n",
      "q_vgg13 accuracy:\t 0.8699\n",
      "q_vgg16 accuracy:\t 0.8745\n",
      "q_vgg19 accuracy:\t 0.8770\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy\n",
    "\n",
    "data_path = \"/workspace/shared/data\"\n",
    "test_dataset = datasets.CIFAR10(root=data_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    accuracy = measure_accuracy(model, test_loader, 'cpu')\n",
    "    print(f'{model_name} accuracy:\\t {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mqbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
