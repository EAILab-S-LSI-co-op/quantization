{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:375: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import copy\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from src.vgg import *\n",
    "from src.util import *\n",
    "import os\n",
    "\n",
    "def quantize_model(model):\n",
    "    quantize_model = copy.deepcopy(model)\n",
    "    quantize_model.qconfig = torch.quantization.get_default_qconfig('x86')\n",
    "    quantize_model = torch.quantization.QuantWrapper(quantize_model)\n",
    "    torch.quantization.prepare(quantize_model, inplace=True)\n",
    "    torch.quantization.convert(quantize_model, inplace=True)\n",
    "    return quantize_model\n",
    "\n",
    "def get_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    return os.path.getsize(\"temp.p\")/1e6\n",
    "    \n",
    "\n",
    "model_path = \"./models\"\n",
    "\n",
    "vgg11 = vgg('vgg11'); vgg11.load_state_dict(torch.load(f'{model_path}/vgg11.pth')); vgg11.eval()\n",
    "vgg13 = vgg('vgg13'); vgg13.load_state_dict(torch.load(f'{model_path}/vgg13.pth')); vgg13.eval()\n",
    "vgg16 = vgg('vgg16'); vgg16.load_state_dict(torch.load(f'{model_path}/vgg16.pth')); vgg16.eval()\n",
    "vgg19 = vgg('vgg19'); vgg19.load_state_dict(torch.load(f'{model_path}/vgg19.pth')); vgg19.eval()\n",
    "\n",
    "qat_vgg11 = quantize_model(vgg11); qat_vgg11.load_state_dict(torch.load(f'{model_path}/qat_vgg11.pth'))\n",
    "qat_vgg13 = quantize_model(vgg13); qat_vgg13.load_state_dict(torch.load(f'{model_path}/qat_vgg13.pth'))\n",
    "qat_vgg16 = quantize_model(vgg16); qat_vgg16.load_state_dict(torch.load(f'{model_path}/qat_vgg16.pth'))\n",
    "qat_vgg19 = quantize_model(vgg19); qat_vgg19.load_state_dict(torch.load(f'{model_path}/qat_vgg19.pth'))\n",
    "\n",
    "# ptq_vgg13 = quantize_model(vgg13); qat_vgg11.load_state_dict(torch.load(f'{model_path}/ptq_static_vgg13.pth'))\n",
    "# ptq_vgg13 = quantize_model(vgg13); qat_vgg11.load_state_dict(torch.load(f'{model_path}/ptq_static_vgg13.pth'))\n",
    "# ptq_vgg16 = quantize_model(vgg16); qat_vgg11.load_state_dict(torch.load(f'{model_path}/ptq_static_vgg16.pth'))\n",
    "# ptq_vgg19 = quantize_model(vgg19); qat_vgg11.load_state_dict(torch.load(f'{model_path}/ptq_static_vgg19.pth'))\n",
    "\n",
    "models = {\n",
    "    'vgg11': vgg11,\n",
    "    'vgg13': vgg13,\n",
    "    'vgg16': vgg16,\n",
    "    'vgg19': vgg19,\n",
    "    \n",
    "    # 'ptq_vgg11': ptq_vgg11,\n",
    "    # 'ptq_vgg13': ptq_vgg13,\n",
    "    # 'ptq_vgg16': ptq_vgg16,\n",
    "    # 'ptq_vgg19': ptq_vgg19,\n",
    "    \n",
    "    'qat_vgg11': qat_vgg11,\n",
    "    'qat_vgg13': qat_vgg13,\n",
    "    'qat_vgg16': qat_vgg16,\n",
    "    'qat_vgg19': qat_vgg19,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg11 model size:\t 36.961198 MB\n",
      "vgg13 model size:\t 37.705754 MB\n",
      "vgg16 model size:\t 58.970204 MB\n",
      "vgg19 model size:\t 80.234732 MB\n",
      "qat_vgg11 model size:\t 9.351926 MB\n",
      "qat_vgg13 model size:\t 9.54994 MB\n",
      "qat_vgg16 model size:\t 14.914609 MB\n",
      "qat_vgg19 model size:\t 20.279278 MB\n"
     ]
    }
   ],
   "source": [
    "# measure model size\n",
    "scale = 1024**2\n",
    "for model_name, model in models.items():\n",
    "    model_size = get_size_of_model(model)\n",
    "    print(f'{model_name} model size:\\t {model_size} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg11 elapsed time:\t 0.002042\n",
      "vgg13 elapsed time:\t 0.002310\n",
      "vgg16 elapsed time:\t 0.003228\n",
      "vgg19 elapsed time:\t 0.004098\n",
      "qat_vgg11 elapsed time:\t 0.000826\n",
      "qat_vgg13 elapsed time:\t 0.001025\n",
      "qat_vgg16 elapsed time:\t 0.001410\n",
      "qat_vgg19 elapsed time:\t 0.001891\n"
     ]
    }
   ],
   "source": [
    "# measure inference latency\n",
    "for model_name, model in models.items():\n",
    "    elapsed_time = measure_inference_latency(model, 'cpu')\n",
    "    print(f'{model_name} elapsed time:\\t {elapsed_time:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "vgg11 accuracy:\t 0.8568\n",
      "vgg13 accuracy:\t 0.8719\n",
      "vgg16 accuracy:\t 0.8728\n",
      "vgg19 accuracy:\t 0.8762\n",
      "qat_vgg11 accuracy:\t 0.8559\n",
      "qat_vgg13 accuracy:\t 0.8684\n",
      "qat_vgg16 accuracy:\t 0.8745\n",
      "qat_vgg19 accuracy:\t 0.8763\n"
     ]
    }
   ],
   "source": [
    "# measure accuracy\n",
    "\n",
    "data_path = \"/workspace/shared/data\"\n",
    "test_dataset = datasets.CIFAR10(root=data_path, train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    accuracy = measure_accuracy(model, test_loader, 'cpu')\n",
    "    print(f'{model_name} accuracy:\\t {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mqbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
